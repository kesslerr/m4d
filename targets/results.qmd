---
title: "Multiverse4Decoding"
author: "Roman Kessler"
format: pdf
prefer-html: true
editor: visual
---

```{r}
library(targets)
library(tarchetypes)
library(dplyr)
library(xtable)
library(ggpubr)
options(dplyr.print_max = 1e9)
base_dir = "/Users/roman/GitHub/m4d/"
manuscript_dir = paste0(base_dir,"manuscript/")

```

## Multiverse for Decoding

In short, decoding accuracies have been calculated for each participant, each experiment, and each variation in the pipeline. Marginal means have been calculated and plotted in the following.

After careful consideration, I re-decided to use the MLM approach similar than Clayson et al (2021, Neuroimage) anyways, despite initial concerns. After doing some simulations, and comparing it to other approaches, the results seem similar and trustworthy.

```{r}
#tar_visnetwork()
```

```{r}
#tar_visnetwork(targets_only=TRUE, label=c("description", "branches"))
```

## Overview of decoding performances

### ERPCORE

```{r, fig.width=8,fig.height=6}
#| echo: false
tar_read(overview)

```

## Time-resolved - Example UNIVERSE result

```{r, fig.width=7,fig.height=10}
#| echo: false
tar_read(timeresolved_luck)
```

## MLM and LM models --> Are interactions necessary?

### EEGNet 

```{r}
r2s = c()
r2si2 = c()

for (i in 1:7){
  modeli2 <- tar_read(eegnet_HLMi2_exp, branches=i)[[1]]
  model <- tar_read(eegnet_HLM_exp, branches=i)[[1]]
  models <- summary(model)
  modeli2s <- summary(modeli2)
  r2s = c(r2s, models$adj.r.squared)
  r2si2 = c(r2si2, modeli2s$adj.r.squared)
}
print("Adj. R2 no interactions:")
print(r2s)
print("Adj. R2 2-way interactions:")
print(r2si2)
```

### Time-resolved

Model fits of models (per experiment) without and with interactions
```{r}
r2s = c()
r2si2 = c()

for (i in 1:7){
  modeli2 <- tar_read(sliding_LMi2_exp, branches=i)[[1]]
  model <- tar_read(sliding_LM_exp, branches=i)[[1]]
  models <- summary(model)
  modeli2s <- summary(modeli2)
  r2s = c(r2s, models$adj.r.squared)
  r2si2 = c(r2si2, modeli2s$adj.r.squared)
}
print("Adj. R2 no interactions:")
print(r2s)
print("Adj. R2 2-way interactions:")
print(r2si2)
```
```{r, fig.width=14,fig.height=10}
ggarrange( tar_read(sliding_LM_qq_comb),
           tar_read(sliding_LMi2_qq_comb),
           ncols=2)
```
Doesn't really seem to lead to improvement of Normality assumtions when including interactions.

Residual vs Fitted

```{r, fig.width=14,fig.height=10}
ggarrange( tar_read(sliding_LM_rvf_comb),
           tar_read(sliding_LMi2_rvf_comb),
           ncols=2)
```
Res vs Fitted generally looks better when including 2-way interactions!!


## Influence of analysis choises - (M)LM and EMM

### Estimated Marginal Means & Differences

#### EEGNET

##### Means

```{r}
tar_read(eegnet_HLM_emm_means_comb)
```

##### Contrasts / Pairwise differences

```{r}
tar_read(eegnet_HLM_emm_contrasts_comb)
```

##### Omnibus F-Test per facet (Experiment-Model and Preprocessing Step)

```{r}
tar_read(eegnet_HLM_emm_omni_comb)

```

#### Time-resolved

##### Means

```{r}
tar_read(sliding_LM_emm_means_comb)
```

##### Contrasts / Pairwise differences

```{r}
tar_read(sliding_LM_emm_contrasts_comb)
```

##### Omnibus F-Test per facet (Experiment-Model and Preprocessing Step)

```{r}
tar_read(sliding_LM_emm_omni_comb)
```

#### 

### Relative performance differences

```{r, fig.width=10,fig.height=10}
#| echo: false
tar_read(heatmaps)
```

## Random Effects

Inspection of RFX parameter estimates (Intercept corresponds to Random Intercept (Subject), and the remaining terms are random slopes by subject.

```{r, fig.width=7,fig.height=7}
#| echo: false
tar_read(eegnet_RFX_plot)

```

## EDA of sociodemographics (EEGNet)

```{r}
tar_read(rfx_demographics)
```

## Model diagnostics

-   Mixed-effects models are somewhat robust to mild deviations from normality, especially with more random effects.

-   Visual inspection of diagnostic plots is often more informative than relying solely on p-values from these tests.

-   Tests often focus on residuals at the observation level, whereas mixed-effects models also have random effects variability to consider.

### QQ-Plots

-   **What to look for:**

    -   Ideally, the majority of the residuals should fall close to the straight diagonal line. This indicates that the residuals are approximately normally distributed.

-   **What you shouldn't see:**

    -   Substantial deviations from the diagonal line, especially in the tails of the distribution. This suggests the normality assumption of the model might be violated.

```{r, fig.width=7,fig.height=7}
#| echo: false
tar_read(eegnet_HLM_qq_comb)
tar_read(sliding_LM_qq_comb)
```

### Residuals vs Fitted - Plots

-   **What to look for:**

    -   **Linearity:** Ideally, there should be no clear pattern in the residuals (they should be randomly scattered around zero). A curved pattern might indicate a nonlinear relationship you haven't modeled.

    -   **Homoscedasticity:** The spread of the residuals should be relatively constant across fitted values. If the spread increases or decreases, it suggests non-constant variance (heteroscedasticity).

    -   **Outliers:** Look for points that lie far away from the majority of the residuals. These could be influential observations.

-   **What you shouldn't see:**

    -   Clear patterns (e.g., U-shapes, funnel shapes).

    -   Significant changes in the spread of residuals across fitted values.

    -   Extreme outliers.

```{r, fig.width=7,fig.height=7}
#| echo: false
tar_read(eegnet_HLM_rvf_comb)
tar_read(sliding_LM_rvf_comb)
```

### Scale-Location-Plots

-   **What to look for:**

    -   Ideally, the line should be relatively horizontal, indicating constant variance (homoscedasticity).

-   **What you shouldn't see:**

    -   Strong trends or patterns in the line suggesting changing variance across fitted values (heteroscedasticity).

```{r, fig.width=7,fig.height=7}
#| echo: false
tar_read(eegnet_HLM_sasrvf_comb)
tar_read(sliding_LM_sasrvf_comb)
```

## 
