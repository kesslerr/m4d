adj_p <- as.numeric(format(adj_p, nsmall = 2)) # make to float with 2 decimals
adj_p
tar_make()
tar_make()
agemodel <- lm(Intercept ~ age * sex, data = data)
agemodel
summary(agemodel)
tar_make()
?ggpairs
??ggpairs
tar_read(rfx)
wide_data <- tar_read(rfx) %>%
pivot_wider(names_from = Experiment, values_from = "Intercept") %>%
select(-c("Subject")) # remove sub for now
??pivot_wider
library(tidyr)
wide_data <- tar_read(rfx) %>%
pivot_wider(names_from = Experiment, values_from = "Intercept") %>%
select(-c("Subject")) # remove sub for now
wide_data
ggpairs(wide_data)
library(GGally)
ggpairs(wide_data)
library(GGally)
library(dplyr)
data(mtcars)
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) {
# Extract x and y variables
x <- data[[deparse(substitute(mapping$x))]]
y <- data[[deparse(substitute(mapping$y))]]
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
p_value_adj <- p.adjust(p_value, method = "bonferroni", n = length(data) * (length(data) - 1) / 2)
# Create a data frame for ggally_statistic
cor_value <- test$estimate
data.frame(
cor = round(cor_value, 2),
p_value = format.pval(p_value_adj, digits = 2)
)
}
# Use ggpairs with the custom correlation function
ggpairs(mtcars, lower = list(continuous = wrap(cor_with_p_adjust)),
upper = list(continuous = "cor"))
mtcars
ggpairs(wide_data, lower = list(continuous = wrap(cor_with_p_adjust)),
upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
?cor.test
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) {
# Extract x and y variables
x <- data[[deparse(substitute(mapping$x))]]
y <- data[[deparse(substitute(mapping$y))]]
print(x)
print(y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
p_value_adj <- p.adjust(p_value, method = "bonferroni", n = length(data) * (length(data) - 1) / 2)
# Create a data frame for ggally_statistic
cor_value <- test$estimate
data.frame(
cor = round(cor_value, 2),
p_value = format.pval(p_value_adj, digits = 2)
)
}
ggpairs(mtcars, lower = list(continuous = wrap(cor_with_p_adjust)),
upper = list(continuous = "cor"))
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) {
# Extract x and y variables
x <- eval_data_col(data, mapping$x)
y <- eval_data_col(data, mapping$y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
# Bonferroni correction: number of comparisons is choose(n, 2)
p_value_adj <- p.adjust(p_value, method = "bonferroni", n = choose(ncol(data), 2))
# Create a label for ggally_text
label <- paste("r = ", round(test$estimate, 2), "\n", "p = ", format.pval(p_value_adj, digits = 2))
# Create ggally_text object
ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...)
}
ggpairs(wide_data, lower = list(continuous = wrap(cor_with_p_adjust)),
upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggpairs(wide_data,
upper = list(continuous = wrap(cor_with_p_adjust))) +
#upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
list(continuous = wrap(cor_with_p_adjust))
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) {
# Extract x and y variables
x <- eval_data_col(data, mapping$x)
y <- eval_data_col(data, mapping$y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
# Bonferroni correction: number of comparisons is choose(n, 2)
p_value_adj <- p.adjust(p_value, method = "bonferroni", n = choose(ncol(data), 2))
# Create a label for ggally_text
label <- paste("r = ", round(test$estimate, 2), "\n", "p = ", format.pval(p_value_adj, digits = 2))
# Create ggally_text object
#ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...)
# also remove grid lines
ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...) +
theme(panel.grid = element_blank())  # Remove gridlines
}
ggpairs(wide_data,
upper = list(continuous = wrap(cor_with_p_adjust))) +
#upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) {
# Extract x and y variables
x <- eval_data_col(data, mapping$x)
y <- eval_data_col(data, mapping$y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
# Bonferroni correction: number of comparisons is choose(n, 2)
p_value_adj <- p.adjust(p_value, method = "bonferroni", n = choose(ncol(data), 2))
# Create a label for ggally_text
label <- paste("r = ", round(test$estimate, 2), "\n", "p = ", format.pval(p_value_adj, digits = 2))
# Create ggally_text object
#ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...)
# also remove grid lines
ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())  # Remove gridlines
}
ggpairs(wide_data,
upper = list(continuous = wrap(cor_with_p_adjust))) +
#upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "tukey", ...) { #pearson
# Extract x and y variables
x <- eval_data_col(data, mapping$x)
y <- eval_data_col(data, mapping$y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
# Bonferroni correction: number of comparisons is choose(n, 2)
p_value_adj <- p.adjust(p_value, method = "bonferroni", n = choose(ncol(data), 2))
# Create a label for ggally_text
label <- paste("r = ", round(test$estimate, 2), "\n", "p = ", format.pval(p_value_adj, digits = 2))
# Create ggally_text object
#ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...)
# also remove grid lines
ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())  # Remove gridlines
}
ggpairs(wide_data,
upper = list(continuous = wrap(cor_with_p_adjust))) +
#upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Custom function to calculate correlations with adjusted p-values
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) { #pearson
# Extract x and y variables
x <- eval_data_col(data, mapping$x)
y <- eval_data_col(data, mapping$y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
# Bonferroni correction: number of comparisons is choose(n, 2)
p_value_adj <- p.adjust(p_value, method = "tukey", n = choose(ncol(data), 2))
# Create a label for ggally_text
label <- paste("r = ", round(test$estimate, 2), "\n", "p = ", format.pval(p_value_adj, digits = 2))
# Create ggally_text object
#ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...)
# also remove grid lines
ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())  # Remove gridlines
}
ggpairs(wide_data,
upper = list(continuous = wrap(cor_with_p_adjust))) +
#upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
cor_with_p_adjust <- function(data, mapping, method = "pearson", ...) { #pearson
# Extract x and y variables
x <- eval_data_col(data, mapping$x)
y <- eval_data_col(data, mapping$y)
# Perform correlation test
test <- cor.test(x, y, method = method)
# Extract p-value and adjust using Bonferroni correction
p_value <- test$p.value
# Bonferroni correction: number of comparisons is choose(n, 2)
p_value_adj <- p.adjust(p_value, method = "BH", n = choose(ncol(data), 2))
# Create a label for ggally_text
label <- paste("r = ", round(test$estimate, 2), "\n", "p = ", format.pval(p_value_adj, digits = 2))
# Create ggally_text object
#ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...)
# also remove grid lines
ggally_text(label = label, color = ifelse(p_value_adj < 0.05, "red", "black"), ...) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())  # Remove gridlines
}
ggpairs(wide_data,
upper = list(continuous = wrap(cor_with_p_adjust))) +
#upper = list(continuous = "cor")) +
#labs(title="Random Intercept Correlation Between Experiments") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
tar_make()
tar_read(forking_path_luck)
tar_read(timeresolved_luck)
tar_make()
tar_make()
tar_make()
tar_make()
tar_read(rfx_demographics_all)
?p.adjust
data <- tar_read(data_eegnet)
data %<>%
filter(subject == "sub-001") %>%
filter(experiment == "N170") %>%
select(-c(subject, accuracy, experiment))
# now change the names of all columns with the replacements
names(data) <- recode(names(data), !!!replacements)
# make long
data_long <- data %>%
make_long(names(data)) %>%
mutate(node = recode(node, !!!replacements)) %>% # also replace with better names
mutate(next_node = recode(next_node, !!!replacements))
?make_long
??make_long
library(ggsankey)
data %<>%
filter(subject == "sub-001") %>%
filter(experiment == "N170") %>%
select(-c(subject, accuracy, experiment))
data <- tar_read(data_eegnet)
data %<>%
filter(subject == "sub-001") %>%
filter(experiment == "N170") %>%
select(-c(subject, accuracy, experiment))
# now change the names of all columns with the replacements
names(data) <- recode(names(data), !!!replacements)
# make long
data_long <- data %>%
make_long(names(data)) %>%
mutate(node = recode(node, !!!replacements)) %>% # also replace with better names
mutate(next_node = recode(next_node, !!!replacements))
# reorder factors in node and next_node
data_long <- data_long %>%
mutate(node = factor(node, levels = rev(c("None", "ICA", "offset", "linear", "False", "True", "average", "Cz", "P9/P10", "200 ms", "400 ms", "6 Hz", "20 Hz", "45 Hz", "0.1 Hz", "0.5 Hz"))),
next_node = factor(next_node, levels = rev(c("None", "ICA", "offset", "linear", "False", "True", "average", "Cz", "P9/P10", "200 ms", "400 ms", "6 Hz", "20 Hz", "45 Hz", "0.1 Hz", "0.5 Hz"))))
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
labs(x = "processing step") + #, title = "Multiverse"
theme(legend.position = "none",
#plot.title = element_text(hjust = .5) # to make it central
)
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
labs(x = "processing step") + #, title = "Multiverse"
theme(legend.position = "none",
plot.margin=grid::unit(c(0,0,0,0), "mm") # remove white space around plot
#plot.title = element_text(hjust = .5) # to make it central
)
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
labs(x = "processing step") + #, title = "Multiverse"
theme(legend.position = "none",
plot.margin=margin(0,0,0,0), #grid::unit(c(0,0,0,0), "mm") # remove white space around plot
#plot.title = element_text(hjust = .5) # to make it central
)
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
labs(x = "processing step") + #, title = "Multiverse"
theme(legend.position = "none",
plot.margin=margin(0,0,0,0), #grid::unit(c(0,0,0,0), "mm") # remove white space around plot
#plot.title = element_text(hjust = .5) # to make it central
) +
scale_x_discrete(position = "top") +          # Move x-axis to the top
coord_cartesian(clip = "off")
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
labs(x = "processing step") + #, title = "Multiverse"
theme(legend.position = "none",
plot.margin=margin(0,0,0,0), #grid::unit(c(0,0,0,0), "mm") # remove white space around plot
#plot.title = element_text(hjust = .5) # to make it central
) +
scale_x_discrete(position = "top") #+          # Move x-axis to the top
#coord_cartesian(clip = "off")
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
#labs(x = "processing step") + #, title = "Multiverse"
theme(legend.position = "none",
plot.margin=margin(0,0,0,0), #grid::unit(c(0,0,0,0), "mm") # remove white space around plot
#plot.title = element_text(hjust = .5) # to make it central
) +
scale_x_discrete(position = "top") #+          # Move x-axis to the top
#coord_cartesian(clip = "off")
ggplot(data_long, aes(x = x, next_x = next_x, node = node, next_node = next_node, fill = factor(node), label = node)) +
geom_sankey(flow.alpha = .6,
node.color = "gray20") +
geom_sankey_label(size = 4, color = "white", fill = "gray40") +
#scale_fill_viridis_d(drop = FALSE) +
#paletteer::scale_fill_paletteer_d("colorBlindness::paletteMartin") +
scale_fill_grey() +
theme_sankey(base_size = 18) +
labs(x = "") + #, title = "Multiverse" processing step
theme(legend.position = "none",
plot.margin=margin(0,0,0,0), #grid::unit(c(0,0,0,0), "mm") # remove white space around plot
#plot.title = element_text(hjust = .5) # to make it central
) +
scale_x_discrete(position = "top") #+          # Move x-axis to the top
#coord_cartesian(clip = "off")
tar_make()
tar_make()
tar_make()
data <- tar_read(data_tsum)
data_fp <- semi_join(data, luckfps,
by = c("experiment", "emc", "mac", "lpf", "hpf", "ref", "base", "det", "ar")) #c("experiment", "ref", "hpf", "lpf", "emc", "mac", "base", "det", "ar")
# TR-Decoding with points as significance markers
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy")
data <- tar_read(data_sliding)
data_fp <- semi_join(data, luckfps,
by = c("experiment", "emc", "mac", "lpf", "hpf", "ref", "base", "det", "ar")) #c("experiment", "ref", "hpf", "lpf", "emc", "mac", "base", "det", "ar")
# TR-Decoding with points as significance markers
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy")
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
timeresolved_plot <- function(data){
data_fp <- semi_join(data, luckfps,
by = c("experiment", "emc", "mac", "lpf", "hpf", "ref", "base", "det", "ar")) #c("experiment", "ref", "hpf", "lpf", "emc", "mac", "base", "det", "ar")
# TR-Decoding with points as significance markers
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy") +
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
theme_minimal()
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy") +
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
theme_minimal()
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy") +
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
theme_void()
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy") +
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
theme_classic()
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy") +
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
#theme_classic()
theme_grey()
ggplot(data_fp, aes(x = times, y = `balanced accuracy`)) +
geom_line() +
geom_hline(yintercept=0.5, linetype="solid") +
geom_vline(xintercept=0, linetype="dashed") +
geom_point(data=filter(data_fp, significance=="TRUE"),
aes(x=times, y=0.48),
color=colors_dark[3],
size=1
) +
facet_wrap(experiment~., scales = "free_x", ncol=1) +
scale_x_continuous(breaks = seq(-8, 8, by = 2)/10,
labels = seq(-8, 8, by = 2)/10) +
labs(x="Time [s]", y="Accuracy") +
#title="Time-Resolved Decoding Results - Exemplary Single Forking Path")
theme_classic()
#theme_grey()
if (diagnostics) {
start <- Sys.time()
profile <- tempfile("renv-startup-", fileext = ".Rprof")
utils::Rprof(profile)
on.exit({
utils::Rprof(NULL)
elapsed <- signif(difftime(Sys.time(), start, units = "auto"), digits = 2L)
writeLines(sprintf("- renv took %s to run the autoloader.", format(elapsed)))
writeLines(sprintf("- Profile: %s", profile))
print(utils::summaryRprof(profile))
}, add = TRUE)
}
source("~/GitHub/m4d/targets/renv/activate.R", echo=TRUE)
source("~/GitHub/m4d/targets/_targets.R", echo=TRUE)
